{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from scipy.stats import distributions as pdf\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import sklearn.datasets as dset\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def make_data(num_samples, scatter):\n",
    "    np.random.seed(123)\n",
    "    class_means =[(1,1),(1,8),(7,1)]\n",
    "    c1_samples =np.random.multivariate_normal(mean=class_means[0], \n",
    "                                             cov=[[scatter[0] *1, 0], [0, scatter[0] *1]], \n",
    "                                            size=num_samples[0])\n",
    "    c2_samples = np.random.multivariate_normal(mean=class_means[1], \n",
    "                                            cov=[[scatter[1] *1, 0], [0, scatter[1] *1]], \n",
    "                                            size=num_samples[1])\n",
    "    c3_samples = np.random.multivariate_normal(mean=class_means[2], \n",
    "                                            cov=[[scatter[2] *1, 0], [0, scatter[2] *1]], \n",
    "                                            size=num_samples[2])\n",
    "    \n",
    "    # labels\n",
    "    c1_labels = np.vstack((np.ones(c1_samples.shape[0]), \n",
    "                           np.zeros(c1_samples.shape[0]), \n",
    "                           np.zeros(c1_samples.shape[0]))).T\n",
    "    c2_labels = np.vstack((np.zeros(c2_samples.shape[0]), \n",
    "                           np.ones(c2_samples.shape[0]), \n",
    "                           np.zeros(c2_samples.shape[0]))).T\n",
    "    c3_labels = np.vstack((np.zeros(c3_samples.shape[0]), \n",
    "                           np.zeros(c3_samples.shape[0]), \n",
    "                           np.ones(c3_samples.shape[0]))).T\n",
    "    \n",
    "    \n",
    "    # feature matrix, weight matrix, label matrix (one-hot)\n",
    "    x = np.vstack((c1_samples, c2_samples, c3_samples))\n",
    "    x = np.hstack((np.ones(x.shape[0])[:,None],x)) # bias term\n",
    "    labels = np.vstack((c1_labels, c2_labels, c3_labels))\n",
    "    \n",
    "    return x, labels, (c1_samples, c2_samples, c3_samples)\n",
    "\n",
    "def plot_data(c1_samples,c2_samples,c3_samples):\n",
    "    ax = plt.subplot()\n",
    "#     c1_samples,c2_samples,c3_samples=np.log(c1_samples),np.log(c2_samples),np.log(c3_samples)\n",
    "    \n",
    "    \n",
    "    ax.scatter(c1_samples[:,0],c1_samples[:,1])\n",
    "    ax.scatter(c2_samples[:,0],c2_samples[:,1])\n",
    "    ax.scatter(c3_samples[:,0],c3_samples[:,1])\n",
    "    return ax\n",
    "\n",
    "\n",
    "def loss_fn(x,weights,labels):    \n",
    "    # scores\n",
    "    s = x@weights.T  # scores\n",
    "    eps = np.max(s)  # constant for numerical stability\n",
    "    probs = (np.exp(s-eps))/ (np.sum(np.exp(s-eps), axis=1)+1e-6)[:,None]\n",
    "    # Loss function (negative log-likelihood)\n",
    "    loss = np.sum(-labels*np.log(probs+1e-6))\n",
    "    dloss_dw = (probs-labels).T@x\n",
    "    return loss, dloss_dw, probs\n",
    "\n",
    "\n",
    "def probability(x,weights):\n",
    "    s = x@weights.T  # scores\n",
    "    eps = np.max(s)  # constant for numerical stability\n",
    "    probs = (np.exp(s-eps))/ (np.sum(np.exp(s-eps), axis=1)+1e-6)[:,None]\n",
    "    return probs\n",
    "\n",
    "\n",
    "def grid(x,w, mode='sep'):\n",
    "    grid_resolution = 120\n",
    "#     x = np.hstack((np.ones(x.shape[0])[:,None],x))\n",
    "    grid_x = np.linspace(np.min(x[:,1]),np.max(x[:,1]),grid_resolution)\n",
    "    grid_y = np.linspace(np.min(x[:,2]),np.max(x[:,2]),grid_resolution)\n",
    "\n",
    "    grid = np.zeros((grid_resolution,grid_resolution))\n",
    "    for i, ig in enumerate(grid_x):\n",
    "        for j, jg in enumerate(grid_y):\n",
    "            feats = np.array((1,ig,jg))[:,None].T\n",
    "            pr=probability(feats,w)\n",
    "            if mode == 'sep':\n",
    "                grid[i,j]=np.argmax(pr)\n",
    "            elif mode == 'probs':\n",
    "                grid[i,j]=np.max(pr)\n",
    "\n",
    "    return grid_x, grid_y, grid\n",
    "  \n",
    "\n",
    "def plot_decision_boundary(ax1, u, v, z, levels=None, colors=['k','k','k']):\n",
    "    # Basic contour plot\n",
    "#     fig1, ax1 = plt.subplots()\n",
    "    if levels == None:\n",
    "        ctp = ax1.contour(u, v, z, alpha=0.6)\n",
    "    else:\n",
    "        ctp = ax1.contour(u, v, z, levels=levels, colors=colors, alpha=0.6)\n",
    "        # Description of contours\n",
    "        fmt = {l: str(l) for l in ctp.levels}\n",
    "        ax1.clabel(ctp, ctp.levels, inline=True, fmt=fmt, fontsize=10)\n",
    "        ax1.contourf(u, v, z, cmap=plt.cm.Paired)\n",
    "    return ax1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c0de8e83cf45faa299e15dfd7e203a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5000, description='num_epochs', max=10000, step=500), Dropdown(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(num_epochs=(0,10000,500), \n",
    "          learning_rate=[0.01,0.02,0.03], \n",
    "          c1_samples='20',\n",
    "          c1_scatter=(1,8,0.4),\n",
    "          c2_samples='20',\n",
    "          c2_scatter=(1,8,0.4),\n",
    "          c3_samples='20',\n",
    "          c3_scatter=(1,8,0.4),)\n",
    "def multi_log_regression(num_epochs, \n",
    "                         learning_rate, \n",
    "                         c1_samples,\n",
    "                         c1_scatter,\n",
    "                         c2_samples, \n",
    "                         c2_scatter,\n",
    "                         c3_samples,\n",
    "                         c3_scatter,):\n",
    "    if  c1_samples.isdigit()==False or c2_samples.isdigit()==False or c2_samples.isdigit()==False:\n",
    "            pass\n",
    "    else:\n",
    "        c1_samples = int(c1_samples)\n",
    "        c2_samples = int(c2_samples)\n",
    "        c3_samples = int(c3_samples)\n",
    "        x, labels, samples = make_data((c1_samples, c2_samples, c3_samples),\n",
    "                                       (c1_scatter, c2_scatter, c3_scatter))\n",
    "#         w = 0.001 * np.ones((3,x.shape[1])) # #classes x features\n",
    "#         for epoch in range(num_epochs):\n",
    "#                 loss, dw, probs = loss_fn(x,w,labels)\n",
    "#                 acc = np.sum(np.argmax(labels)==np.argmax(probs))\n",
    "#                 w -= learning_rate*dw\n",
    "#         u,v,z = grid(x,w,mode='sep')\n",
    "#         acc = np.sum(np.argmax(labels, axis=1)==np.argmax(probs, axis=1))\n",
    "#         acc /= x.shape[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "#         ax = plot_data(*samples)\n",
    "        \n",
    "#         # Plot missclassified data\n",
    "#         if acc != 1.0:\n",
    "#             idx = np.argmax(labels, axis=1)!=np.argmax(probs, axis=1)\n",
    "#             ax.scatter(x[idx,1], x[idx,2], marker='x', c='k')\n",
    "        \n",
    "        \n",
    "#         try: \n",
    "#             ax.text(9,10,'loss: {0:.4f}'.format(loss))\n",
    "#             ax.text(9,11,'acc: {0:.4f}'.format(acc)) \n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#         ax = plot_decision_boundary(ax, u,v,z.T)\n",
    "\n",
    "        \n",
    "#         plt.xlim(-4,14)\n",
    "#         plt.ylim(-4,14)\n",
    "#         ax.figure\n",
    "    for multi_class in (['multinomial']):\n",
    "        clf = LogisticRegression(solver='sag', max_iter=num_epochs, random_state=42,\n",
    "                                 multi_class=multi_class).fit(x, np.argmax(labels, axis=1))\n",
    "\n",
    "        # print the training scores\n",
    "        #print(\"training score : %.3f (%s)\" % (clf.score(x, np.argmax(labels, axis=1)), multi_class))\n",
    "\n",
    "        # create a mesh to plot in\n",
    "        h = .02  # step size in the mesh\n",
    "        x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "        y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                             np.arange(y_min, y_max, h))\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        # Put the result into a color plot\n",
    "        print(Z[40:70])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "        ids = clf.predict(x) != np.argmax(labels, axis=1)\n",
    "        plt.scatter(x[ids,0], x[ids,1], marker='x', c='k')\n",
    "        # Plot also the training points\n",
    "        colors = \"bry\"\n",
    "        for i, color in zip(clf.classes_, colors):\n",
    "            idx = np.where(np.argmax(labels, axis=1) == i)\n",
    "            plt.scatter(x[idx, 0], x[idx, 1], c=color, cmap=plt.cm.Paired,\n",
    "                        edgecolor='black', s=20)\n",
    "        \n",
    "#     return ax\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4321f9d5554f80b7f739e86f2335c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5000, description='num_epochs', max=10000, step=500), Dropdown(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(num_epochs=(0,10000,500), \n",
    "          learning_rate=[0.01,0.02,0.03], \n",
    "          c1_samples='20',\n",
    "          c1_scatter=(1,8,0.4),\n",
    "          c2_samples='20',\n",
    "          c2_scatter=(1,8,0.4),\n",
    "          c3_samples='20',\n",
    "          c3_scatter=(1,8,0.4),)\n",
    "def multi_log_regression(num_epochs, \n",
    "                         learning_rate, \n",
    "                         c1_samples,\n",
    "                         c1_scatter,\n",
    "                         c2_samples, \n",
    "                         c2_scatter,\n",
    "                         c3_samples,\n",
    "                         c3_scatter,):\n",
    "    if  c1_samples.isdigit()==False or c2_samples.isdigit()==False or c2_samples.isdigit()==False:\n",
    "            pass\n",
    "    else:\n",
    "        c1_samples = int(c1_samples)\n",
    "        c2_samples = int(c2_samples)\n",
    "        c3_samples = int(c3_samples)\n",
    "        x, labels, samples = make_data((c1_samples, c2_samples, c3_samples),\n",
    "                                       (c1_scatter, c2_scatter, c3_scatter))\n",
    "        \n",
    "#         # Denis\n",
    "        w = 0.001 * np.ones((3,x.shape[1])) # #classes x features\n",
    "        for epoch in range(num_epochs):\n",
    "                loss, dw, probs = loss_fn(x,w,labels)\n",
    "                acc = np.sum(np.argmax(labels)==np.argmax(probs))\n",
    "                w -= learning_rate*dw\n",
    "#         u,v,z = grid(x,w,mode='sep')\n",
    "\n",
    "        \n",
    "    # SK\n",
    "    \n",
    "    clf = LogisticRegression(solver='sag', max_iter=num_epochs, random_state=42,\n",
    "                             multi_class='multinomial').fit(x, np.argmax(labels, axis=1))\n",
    "    print(probs)\n",
    "\n",
    "#     w = np.hstack((clf.intercept_[:,None],clf.coef_))\n",
    "    u,v,z = grid(x,w,mode='sep')\n",
    "    ax = plot_data(*samples)\n",
    "\n",
    "\n",
    "    try: \n",
    "        ax.text(9,10,'loss: {0:.4f}'.format(loss))\n",
    "        ax.text(9,11,'acc: {0:.4f}'.format(acc)) \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    ax = plot_decision_boundary(ax, u,v,z.T)\n",
    "\n",
    "\n",
    "    plt.xlim(-8,12)\n",
    "    plt.ylim(-8,15)\n",
    "\n",
    "\n",
    "    ax.figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# for multi_class in ('multinomial', 'ovr'):\n",
    "#     clf = LogisticRegression(solver='sag', max_iter=100, random_state=42,\n",
    "#                              multi_class=multi_class).fit(x, labels)\n",
    "\n",
    "#     # print the training scores\n",
    "#     print(\"training score : %.3f (%s)\" % (clf.score(x, labels), multi_class))\n",
    "\n",
    "#     # create a mesh to plot in\n",
    "#     h = .02  # step size in the mesh\n",
    "#     x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "#     y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "#     xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "#                          np.arange(y_min, y_max, h))\n",
    "\n",
    "#     # Plot the decision boundary. For that, we will assign a color to each\n",
    "#     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "#     Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "#     # Put the result into a color plot\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "#     plt.figure()\n",
    "#     plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "#     plt.title(\"Decision surface of LogisticRegression (%s)\" % multi_class)\n",
    "#     plt.axis('tight')\n",
    "\n",
    "#     # Plot also the training points\n",
    "#     colors = \"bry\"\n",
    "#     for i, color in zip(clf.classes_, colors):\n",
    "#         idx = np.where(y == i)\n",
    "#         plt.scatter(x[idx, 0], x[idx, 1], c=color, cmap=plt.cm.Paired,\n",
    "#                     edgecolor='black', s=20)\n",
    "\n",
    "#     # Plot the three one-against-all classifiers\n",
    "#     xmin, xmax = plt.xlim()\n",
    "#     ymin, ymax = plt.ylim()\n",
    "#     coef = clf.coef_\n",
    "#     intercept = clf.intercept_\n",
    "\n",
    "#     def plot_hyperplane(c, color):\n",
    "#         def line(x0):\n",
    "#             return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n",
    "#         plt.plot([xmin, xmax], [line(xmin), line(xmax)],\n",
    "#                  ls=\"--\", color=color)\n",
    "\n",
    "#     for i, color in zip(clf.classes_, colors):\n",
    "#         plot_hyperplane(i, color)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'CClass' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ced1d9bae0de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'CClass' object is not callable"
     ]
    }
   ],
   "source": [
    "np.c_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
